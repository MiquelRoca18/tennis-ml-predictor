"""
Ensemble Stacking - Combina modelos con meta-learner
Mejora esperada: Accuracy +0.5% a +1.0%, Brier -0.01 a -0.015
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, brier_score_loss, roc_auc_score
import joblib
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def crear_y_entrenar_stacking(X_train, y_train, X_test, y_test):
    """
    Crea y entrena un Stacking Ensemble con los 3 mejores modelos
    """
    
    logger.info("=" * 70)
    logger.info("ðŸš€ STACKING ENSEMBLE - Meta-Learning")
    logger.info("=" * 70)
    
    # Crear modelos base desde cero (no usar calibrados pre-entrenados)
    logger.info("\nðŸ”§ Creando modelos base...")
    
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    try:
        from xgboost import XGBClassifier
        XGBOOST_AVAILABLE = True
    except ImportError:
        XGBOOST_AVAILABLE = False
        logger.error("   âŒ XGBoost no disponible")
        return None
    
    # Random Forest
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=20,
        random_state=42,
        n_jobs=-1
    )
    logger.info("   âœ… Random Forest inicializado")
    
    # XGBoost con mejores parÃ¡metros del tuning
    xgb = XGBClassifier(
        n_estimators=200,
        max_depth=3,
        learning_rate=0.05,
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_weight=1,
        gamma=0.1,
        random_state=42,
        n_jobs=-1,
        eval_metric='logloss'
    )
    logger.info("   âœ… XGBoost inicializado")
    
    # Gradient Boosting
    gb = GradientBoostingClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    )
    logger.info("   âœ… Gradient Boosting inicializado")
    
    # Crear stacking
    logger.info("\nðŸ”§ Creando Stacking Classifier...")
    estimators = [
        ('rf', rf),
        ('xgb', xgb),
        ('gb', gb)
    ]
    
    # Meta-learner: Logistic Regression
    stacking = StackingClassifier(
        estimators=estimators,
        final_estimator=LogisticRegression(max_iter=1000, random_state=42),
        cv=5,  # 5-fold cross-validation
        n_jobs=-1,
        verbose=1
    )
    
    logger.info("   Estimadores base: Random Forest, XGBoost, Gradient Boosting")
    logger.info("   Meta-learner: Logistic Regression")
    logger.info("   Cross-validation: 5 folds")
    
    # Entrenar
    logger.info("\nðŸ”„ Entrenando Stacking Ensemble...")
    logger.info("   (Esto puede tomar varios minutos...)")
    
    try:
        stacking.fit(X_train, y_train)
        logger.info("   âœ… Entrenamiento completado")
    except Exception as e:
        logger.error(f"   âŒ Error durante entrenamiento: {e}")
        return None
    
    # Evaluar
    logger.info("\nðŸ”® Evaluando en Test Set...")
    y_pred = stacking.predict(X_test)
    y_prob = stacking.predict_proba(X_test)[:, 1]
    
    accuracy = accuracy_score(y_test, y_pred)
    brier = brier_score_loss(y_test, y_prob)
    auc = roc_auc_score(y_test, y_prob)
    
    logger.info("\n" + "=" * 70)
    logger.info("ðŸŽ¯ RESULTADOS DEL STACKING ENSEMBLE")
    logger.info("=" * 70)
    logger.info(f"\nðŸ“Š MÃ©tricas en Test Set:")
    logger.info(f"   Accuracy:     {accuracy*100:.2f}%")
    logger.info(f"   Brier Score:  {brier:.4f}")
    logger.info(f"   AUC-ROC:      {auc:.4f}")
    
    
    # ComparaciÃ³n con weighted ensemble si existe
    logger.info(f"\nðŸ“ˆ ComparaciÃ³n con Weighted Ensemble:")
    try:
        weighted_metrics = pd.read_csv('resultados/weighted_ensemble_metrics.csv', index_col=0)
        weighted_acc = weighted_metrics.loc['accuracy'] * 100
        weighted_brier = weighted_metrics.loc['brier_score']
        
        logger.info(f"\n   Weighted Ensemble:")
        logger.info(f"      Accuracy: {weighted_acc:.2f}% â†’ Stacking: {accuracy*100:.2f}% ({(accuracy*100-weighted_acc):+.2f}%)")
        logger.info(f"      Brier:    {weighted_brier:.4f} â†’ Stacking: {brier:.4f} ({brier-weighted_brier:+.4f})")
    except:
        logger.info("   (Weighted ensemble no encontrado para comparaciÃ³n)")
    
    
    # VerificaciÃ³n de criterios
    logger.info("\n" + "=" * 70)
    logger.info("âœ… VERIFICACIÃ“N DE CRITERIOS")
    logger.info("=" * 70)
    
    objetivo_accuracy = 70.0
    objetivo_brier = 0.18
    
    cumple_accuracy = accuracy * 100 >= objetivo_accuracy
    cumple_brier = brier < objetivo_brier
    
    logger.info(f"\nðŸŽ¯ Objetivos:")
    logger.info(f"   Accuracy â‰¥ {objetivo_accuracy}%:  {'âœ… SÃ' if cumple_accuracy else 'âŒ NO'} ({accuracy*100:.2f}%)")
    logger.info(f"   Brier < {objetivo_brier}:      {'âœ… SÃ' if cumple_brier else 'âŒ NO'} ({brier:.4f})")
    
    if cumple_accuracy and cumple_brier:
        logger.info("\nðŸŽ‰ Â¡Ã‰XITO COMPLETO! Ambos objetivos alcanzados")
        logger.info("   â†’ Proceder a Fase 4")
    elif cumple_accuracy or brier < 0.19:
        logger.info("\nâœ… Â¡EXCELENTE! Muy cerca de los objetivos")
        logger.info("   â†’ Resultado aceptable para producciÃ³n")
    else:
        logger.info("\nâš ï¸  Considerar implementar features adicionales")
        logger.info("   â†’ Ver features_momentum.py en plan de mejora")
    
    # Guardar modelo
    logger.info("\nðŸ’¾ Guardando modelo...")
    Path("modelos").mkdir(exist_ok=True)
    joblib.dump(stacking, 'modelos/stacking_ensemble.pkl')
    logger.info("   âœ… Modelo guardado: modelos/stacking_ensemble.pkl")
    
    # Guardar predicciones
    resultados_df = pd.DataFrame({
        'prob_stacking': y_prob,
        'pred_stacking': y_pred,
        'y_true': y_test
    })
    
    Path("resultados").mkdir(exist_ok=True)
    resultados_df.to_csv('resultados/stacking_ensemble_predictions.csv', index=False)
    logger.info("   âœ… Predicciones guardadas: resultados/stacking_ensemble_predictions.csv")
    
    # Guardar mÃ©tricas
    metricas = {
        'accuracy': accuracy,
        'brier_score': brier,
        'auc_roc': auc,
        'cumple_accuracy': cumple_accuracy,
        'cumple_brier': cumple_brier
    }
    
    pd.Series(metricas).to_csv('resultados/stacking_ensemble_metrics.csv')
    logger.info("   âœ… MÃ©tricas guardadas: resultados/stacking_ensemble_metrics.csv")
    
    logger.info("\nâœ… Stacking Ensemble completado!")
    
    return stacking


if __name__ == "__main__":
    # Cargar datos
    logger.info("ðŸ“‚ Cargando dataset...")
    df = pd.read_csv("datos/processed/dataset_features_fase3_completas.csv")
    df['fecha'] = pd.to_datetime(df['fecha'])
    df = df.sort_values('fecha').reset_index(drop=True)
    
    logger.info(f"   Dataset: {len(df)} partidos")
    
    # Cargar features seleccionadas
    try:
        selected_features = pd.read_csv('resultados/selected_features.txt', header=None)[0].tolist()
        logger.info(f"   âœ… {len(selected_features)} features seleccionadas cargadas")
    except Exception as e:
        logger.error(f"   âŒ Error cargando features: {e}")
        logger.error("   â†’ Ejecutar primero: python run_fase3_optimization.py")
        exit(1)
    
    # Split: 60% train, 20% val, 20% test
    n = len(df)
    train_end = int(n * 0.6)
    val_end = int(n * 0.8)
    
    X_train = df.iloc[:train_end][selected_features]
    y_train = df.iloc[:train_end]['resultado']
    X_test = df.iloc[val_end:][selected_features]
    y_test = df.iloc[val_end:]['resultado']
    
    logger.info(f"   Train set: {len(X_train)} partidos")
    logger.info(f"   Test set:  {len(X_test)} partidos")
    
    # Crear y entrenar stacking
    stacking_model = crear_y_entrenar_stacking(X_train, y_train, X_test, y_test)
    
    if stacking_model:
        logger.info("\nðŸŽŠ Â¡Proceso completado exitosamente!")
    else:
        logger.error("\nâŒ Error en el proceso de stacking")
