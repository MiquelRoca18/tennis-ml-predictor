"""
Pipeline completo de optimizaci√≥n - Fase 3
Orden correcto: Feature Selection ‚Üí Model Comparison ‚Üí Hyperparameter Tuning ‚Üí Validation
"""
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import sys
import warnings

# Suprimir warnings para output m√°s limpio
warnings.filterwarnings('ignore')

sys.path.insert(0, str(Path(__file__).parents[2]))

from src.models.comparacion_modelos import ModelComparator
from src.models.hyperparameter_tuning import tune_xgboost, tune_lightgbm
from src.models.feature_selection import FeatureSelector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    """
    Ejecuta el pipeline completo de optimizaci√≥n de Fase 3
    ORDEN CORRECTO: Feature Selection ‚Üí Comparaci√≥n ‚Üí Tuning ‚Üí Validaci√≥n
    """
    
    logger.info("=" * 70)
    logger.info("üöÄ FASE 3 - PIPELINE DE OPTIMIZACI√ìN COMPLETO")
    logger.info("=" * 70)
    
    # ========================================================================
    # PASO 0: Cargar datos
    # ========================================================================
    logger.info("\nüìÇ PASO 0: Cargando dataset...")
    df = pd.read_csv("datos/processed/dataset_features_fase3_completas.csv")
    df['fecha'] = pd.to_datetime(df['fecha'])
    df = df.sort_values('fecha').reset_index(drop=True)
    
    logger.info(f"   Dataset: {len(df)} partidos")
    
    # Todas las features disponibles
    all_features = [col for col in df.columns if col not in ['resultado', 'fecha']]
    logger.info(f"   Features totales: {len(all_features)}")
    
    # Split: 60% train, 20% val, 20% test
    n = len(df)
    train_end = int(n * 0.6)
    val_end = int(n * 0.8)
    
    # ========================================================================
    # PASO 1: Feature Selection (PRIMERO)
    # ========================================================================
    logger.info("\n" + "=" * 70)
    logger.info("üéØ PASO 1: FEATURE SELECTION")
    logger.info("=" * 70)
    
    X_train_all = df.iloc[:train_end][all_features]
    y_train = df.iloc[:train_end]['resultado']
    
    selector = FeatureSelector(X_train_all, y_train)
    
    # M√©todo 1: Tree-based
    imp_tree = selector.feature_importance_tree_based()
    
    # M√©todo 2: F-statistic
    imp_f = selector.feature_importance_statistical('f_classif')
    
    # M√©todo 3: Mutual Information
    imp_mi = selector.feature_importance_statistical('mutual_info')
    
    # Comparar m√©todos
    features_comunes = selector.comparar_metodos()
    
    # Eliminar correlacionadas
    X_sin_corr, dropped = selector.eliminar_features_correlacionadas(threshold=0.9)
    
    # Seleccionar top 30
    X_reducido, selected_features = selector.seleccionar_mejores_k(k=30, method='tree_based')
    
    # Guardar features seleccionadas
    if selected_features:
        Path("resultados").mkdir(exist_ok=True)
        pd.Series(selected_features).to_csv('resultados/selected_features.txt', index=False, header=False)
        logger.info("\nüíæ Features seleccionadas guardadas: resultados/selected_features.txt")
    
    # ========================================================================
    # PASO 2: Preparar datos con features seleccionadas
    # ========================================================================
    logger.info(f"\nüìä Usando {len(selected_features)} features seleccionadas")
    
    X_train = df.iloc[:train_end][selected_features]
    X_val = df.iloc[train_end:val_end][selected_features]
    X_test = df.iloc[val_end:][selected_features]
    y_val = df.iloc[train_end:val_end]['resultado']
    y_test = df.iloc[val_end:]['resultado']
    
    logger.info(f"   Splits: Train {len(X_train)} | Val {len(X_val)} | Test {len(X_test)}")
    
    # ========================================================================
    # PASO 3: Comparaci√≥n de modelos
    # ========================================================================
    logger.info("\n" + "=" * 70)
    logger.info("üìä PASO 2: COMPARACI√ìN DE MODELOS")
    logger.info("=" * 70)
    
    comparador = ModelComparator()
    comparador.inicializar_modelos_default()
    
    resultados = comparador.entrenar_y_evaluar(
        X_train, y_train,
        X_val, y_val,
        X_test, y_test
    )
    
    mejor_modelo, df_res = comparador.comparar_resultados()
    
    # ========================================================================
    # PASO 4: Hyperparameter tuning
    # ========================================================================
    logger.info("\n" + "=" * 70)
    logger.info("‚öôÔ∏è  PASO 3: HYPERPARAMETER TUNING")
    logger.info("=" * 70)
    
    # Tune XGBoost
    try:
        logger.info("\nüîß Optimizando XGBoost...")
        mejor_xgb = tune_xgboost(X_train, y_train)
    except Exception as e:
        logger.error(f"‚ùå Error en tuning XGBoost: {e}")
    
    # Tune LightGBM
    try:
        logger.info("\nüîß Optimizando LightGBM...")
        mejor_lgbm = tune_lightgbm(X_train, y_train)
    except Exception as e:
        logger.error(f"‚ùå Error en tuning LightGBM: {e}")
    
    # ========================================================================
    # RESUMEN FINAL
    # ========================================================================
    logger.info("\n" + "=" * 70)
    logger.info("üéâ PIPELINE COMPLETADO")
    logger.info("=" * 70)
    
    logger.info("\nüìã Archivos generados:")
    logger.info("   ‚úÖ resultados/selected_features.txt - 30 features seleccionadas")
    logger.info("   ‚úÖ resultados/feature_importance_tree.png - Importancia de features")
    logger.info("   ‚úÖ resultados/model_comparison.png - Comparaci√≥n de modelos")
    logger.info("   ‚úÖ modelos/*_calibrado.pkl - Modelos calibrados")
    logger.info("   ‚úÖ modelos/*_optimizado.pkl - Modelos optimizados")
    
    logger.info("\nüéØ Pr√≥ximo paso:")
    logger.info("   python src/models/validacion_final_fase3.py")
    logger.info("\n‚úÖ ¬°Listo para validaci√≥n!")


if __name__ == "__main__":
    main()
